<!DOCTYPE html>
<html lang="en">
<head>
    <div style="text-align:center"> <h1>BIG DATA:SPARK</h1></div>
  </head>
    <body>
        <h3> List of Experiments: </h3>
1. <br>
<h3>EXPERIMENT-1 : </h3>
To Study of Big Data Analytics and Hadoop Architecture <br>
(i) know the concept of big data architecture <br>
(ii) know the concept of Hadoop architecture <br>
https://infyspringboard.onwingspan.com/web/en/app/toc/lex_auth_01256841991858585686_shared/overview<br><br>
2. Loading DataSet in to HDFS for Spark Analysis <br>
<h3>EXPERIMENT-2 : </h3>
Installation of Hadoop and cluster management <br>
(i) Installing Hadoop single node cluster in ubuntu environment <br>
(ii) Knowing the differencing between single node clusters and multi-node clusters <br>
(iii) Accessing WEB-UI and the port number <br>
(iv) Installing and accessing the environments such as hive and sqoop <br>
(We can restrict to standalone or if the facilities available can try for pseudo-distribution mode {or} fully distribution mode) <br>
https://infyspringboard.onwingspan.com/web/en/viewer/video/lex_auth_01329503531746099243539_shared?collectionId=lex_auth_01329503580490137643544_shared&collectionType=Collection<br><br>
<h3>EXPERIMENT-3 : </h3>
File management tasks & Basic linux commands <br>
(i) Creating a directory in HDFS <br>
(ii) Moving forth and back to directories <br>
(iii) Listing directory contents <br>
(iv) Uploading and downloading a file in HDFS <br>
(v) Checking the contents of the file <br>
(vi) Copying and moving files <br>
(vii) Copying and moving files between local to HDFS environment <br>
(viii) Removing files and paths <br>
(ix) Displaying few lines of a file <br>
(x) Display the aggregate length of a file <br>
(xi) Checking the permissions of a file <br>
(xii) Zipping and unzipping the files with & without permission pasting it to a location <br>
(xiii) Copy, Paste commands <br>
https://infyspringboard.onwingspan.com/web/en/viewer/video/lex_auth_01257430580733542457_shared?collectionId=lex_auth_01256841991858585686_shared&collectionType=Course<br><br>
<h3>EXPERIMENT-4 : </h3>
Map-reducing <br>
(i) Definition of Map-reduce <br>
(ii) Its stages and terminologies<br> 
(iii) Word-count program to understand map-reduce <br>
(Mapper phase, Reducer phase, Driver code) <br>
https://infyspringboard.onwingspan.com/web/en/viewer/video/lex_auth_01257421539761356848_shared?collectionId=lex_auth_01256841991858585686_shared&collectionType=Course<br><br>
<h3>EXPERIMENT-5 : </h3>
Implementing Matrix-Multiplication with Hadoop Map-reduce <br><br>
<h3>EXPERIMENT-6 : </h3>
Compute Average Salary and Total Salary by Gender for an Enterprise. <br><br>
<h3>EXPERIMENT-7 : </h3>
(i) Creating hive tables(External and internal) <br>
 (ii) Loading data to external hive tables from sql tables(or)Structured c.s.v using scoop <br>
(iii) Performing operations like filterations and updations <br>
(iv) Performing Join(inner, outer etc) <br>
(v) Writing User defined function on hive tables <br>
https://infyspringboard.onwingspan.com/web/en/viewer/webmodule/lex_auth_01257841711418572848_shared?collectionId=lex_auth_01258388119638835242_shared&collectionType=Course<br><br>
<h3>EXPERIMENT-8 : </h3>
Create a sql table of employees <br>
Employee table with id,designation <br>
Salary table (salary ,dept id) <br>
Create external table in hive with similar schema of above tables,Move data to hive using scoop and load the contents into tables,filter a new table and write a UDF to encrypt the table with AES-algorithm, Decrypt it with key to show contents <br>
https://infyspringboard.onwingspan.com/web/en/viewer/web module/lex_auth_012606909641981952143_shared?collectionId=lex_auth_0126052684230082561692_shared&collectionType=Course<br><br>
<h3>EXPERIMENT-9 : </h3>
(i) Pyspark Definition(Apache Pyspark) and difference between Pyspark, Scala, pandas <br>
(ii) Pyspark files and class methods <br>
(i) get(file name) <br>
(ii) get root directory() <br><br>
<h3>EXPERIMENT-10 : </h3>
Pyspark -RDD’S <br>
(i) what is RDD’s? <br>
(ii) ways to Create RDD<br> 
(i) parallelized collections <br>
(ii) external dataset <br>
(iii) existing RDD’s <br>
(iv) Spark RDD’s operations <br>
(Count, foreach(), Collect, join,Cache() <br>
https://infyspringboard.onwingspan.com/web/en/app/toc/lex_3509975869549336000_shared/overviw<br><br>
 <h3>EXPERIMENT-11 : </h3>
Perform pyspark transformations <br>
(i) map and flatMap <br>
(ii) to remove the words, which are not necessary to analyze this text. <br>
(iii) groupBy <br>
(iv) What if we want to calculate how many times each word is coming in corpus ? <br>
(v) .How do I perform a task (say count the words ‘spark’ and ‘apache’ in rdd3) separatly on each partition and get the output of the task performed in these partition ? <br>
(vi) unions of RDD <br>
(vii) join two pairs of RDD Based upon their key <br>
https://infyspringboard.onwingspan.com/web/en/app/toc/lex_auth_01330150584451891225182_shared/overview<br><br>
<h3>EXPERIMENT-12: </h3>
Pyspark sparkconf-Attributes and applications <br>
(i) What is Pyspark spark conf () <br>
(ii) Using spark conf create a spark session to write a dataframe to read details in a c.s.v and later move that c.s.v to another location <br><br>
 
Hardware and software configuration: <br>
Hardware Configuration for each Node (Suggested) <br><br>
 Operating System: <br>
 NAME="Red Hat Enterprise Linux Server", VERSION="7.9 (Maipo)" <br><br>
 CPU Architecture: x86_64 <br><br>
 CPU op-mode(s): 32-bit, 64-bit <br><br>
 CPU(s): 32<br><br>
 RAM: 64G <br><br>
Software Required <br><br>
 Java: openjdk version "1.8.0_202" ( installed in all nodes) <br><br>
 For Hive metastore – we have installed MySQL <br><br>
 MySQL: Ver 8.0.19 for Linux on x86_64 (MySQL Community Server - GPL) <br><br>
Resource Links for installation <br><br>
x Hadoop Installation steps: Apache Hadoop 3.3.2 – Hadoop: Setting up a Single Node Cluster. & <br>
Apache Hadoop 3.3.2 – Hadoop Cluster Setup<br><br>
x Links to download different versions of Hadoop : Index of /hadoop/ common (apache.org) &Index of /dist/hadoop/core (apache.org)<br><br>

<h4>Text Books: </h4>
1. Spark in Action, Marko Bonaci and Petar Zecevic, Manning. <br>
2. PySpark SQL Recipes: With HiveQL, Dataframe and Graphframes, Raju Kumar Mishra and Sundar Rajan Raman, Apress Media. <br><br>
 
<h4>Web Links: </h4>
1.https://infyspringboard.onwingspan.com/web/en/app/toc/lex_auth_01330150584451891225182_shared/overview<br>
2.https://infyspringboard.onwingspan.com/web/en/app/toc/lex_auth_01258388119638835242_shared/overview<br>
3.https://infyspringboard.onwingspan.com/web/en/app/toc/lex_auth_0126052684230082561692_shared/overview<br>

<button><a href="BIG DATA-SPARK.docx">BIG DATA-SPARK LAB MANUAL</a></button>

    </body>
</html>
